
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{softmax}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \hypertarget{this-is-the-softmax-workbook-for-ece-c147c247-assignment-2}{%
\subsection{This is the softmax workbook for ECE C147/C247 Assignment
\#2}\label{this-is-the-softmax-workbook-for-ece-c147c247-assignment-2}}

Please follow the notebook linearly to implement a softmax classifier.

Please print out the workbook entirely when completed.

We thank Serena Yeung \& Justin Johnson for permission to use code
written for the CS 231n class (cs231n.stanford.edu). These are the
functions in the cs231n folders and code in the jupyer notebook to
preprocess and show the images. The classifiers used are based off of
code prepared for CS 231n as well.

The goal of this workbook is to give you experience with training a
softmax classifier.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}1}]:} \PY{k+kn}{import} \PY{n+nn}{random}
        \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
        \PY{k+kn}{from} \PY{n+nn}{cs231n}\PY{n+nn}{.}\PY{n+nn}{data\PYZus{}utils} \PY{k}{import} \PY{n}{load\PYZus{}CIFAR10}
        \PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
        
        \PY{o}{\PYZpc{}}\PY{k}{matplotlib} inline
        \PY{o}{\PYZpc{}}\PY{k}{load\PYZus{}ext} autoreload
        \PY{o}{\PYZpc{}}\PY{k}{autoreload} 2
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}2}]:} \PY{k}{def} \PY{n+nf}{get\PYZus{}CIFAR10\PYZus{}data}\PY{p}{(}\PY{n}{num\PYZus{}training}\PY{o}{=}\PY{l+m+mi}{49000}\PY{p}{,} \PY{n}{num\PYZus{}validation}\PY{o}{=}\PY{l+m+mi}{1000}\PY{p}{,} \PY{n}{num\PYZus{}test}\PY{o}{=}\PY{l+m+mi}{1000}\PY{p}{,} \PY{n}{num\PYZus{}dev}\PY{o}{=}\PY{l+m+mi}{500}\PY{p}{)}\PY{p}{:}
            \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{    Load the CIFAR\PYZhy{}10 dataset from disk and perform preprocessing to prepare}
        \PY{l+s+sd}{    it for the linear classifier. These are the same steps as we used for the}
        \PY{l+s+sd}{    SVM, but condensed to a single function.  }
        \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
            \PY{c+c1}{\PYZsh{} Load the raw CIFAR\PYZhy{}10 data}
            \PY{n}{cifar10\PYZus{}dir} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{/Users/ApplePro/Desktop/School/GradSchool/Courses/C247/hw2/cifar\PYZhy{}10\PYZhy{}batches\PYZhy{}py}\PY{l+s+s1}{\PYZsq{}} \PY{c+c1}{\PYZsh{} You need to update this line}
            \PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}test} \PY{o}{=} \PY{n}{load\PYZus{}CIFAR10}\PY{p}{(}\PY{n}{cifar10\PYZus{}dir}\PY{p}{)}
            
            \PY{c+c1}{\PYZsh{} subsample the data}
            \PY{n}{mask} \PY{o}{=} \PY{n+nb}{list}\PY{p}{(}\PY{n+nb}{range}\PY{p}{(}\PY{n}{num\PYZus{}training}\PY{p}{,} \PY{n}{num\PYZus{}training} \PY{o}{+} \PY{n}{num\PYZus{}validation}\PY{p}{)}\PY{p}{)}
            \PY{n}{X\PYZus{}val} \PY{o}{=} \PY{n}{X\PYZus{}train}\PY{p}{[}\PY{n}{mask}\PY{p}{]}
            \PY{n}{y\PYZus{}val} \PY{o}{=} \PY{n}{y\PYZus{}train}\PY{p}{[}\PY{n}{mask}\PY{p}{]}
            \PY{n}{mask} \PY{o}{=} \PY{n+nb}{list}\PY{p}{(}\PY{n+nb}{range}\PY{p}{(}\PY{n}{num\PYZus{}training}\PY{p}{)}\PY{p}{)}
            \PY{n}{X\PYZus{}train} \PY{o}{=} \PY{n}{X\PYZus{}train}\PY{p}{[}\PY{n}{mask}\PY{p}{]}
            \PY{n}{y\PYZus{}train} \PY{o}{=} \PY{n}{y\PYZus{}train}\PY{p}{[}\PY{n}{mask}\PY{p}{]}
            \PY{n}{mask} \PY{o}{=} \PY{n+nb}{list}\PY{p}{(}\PY{n+nb}{range}\PY{p}{(}\PY{n}{num\PYZus{}test}\PY{p}{)}\PY{p}{)}
            \PY{n}{X\PYZus{}test} \PY{o}{=} \PY{n}{X\PYZus{}test}\PY{p}{[}\PY{n}{mask}\PY{p}{]}
            \PY{n}{y\PYZus{}test} \PY{o}{=} \PY{n}{y\PYZus{}test}\PY{p}{[}\PY{n}{mask}\PY{p}{]}
            \PY{n}{mask} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{choice}\PY{p}{(}\PY{n}{num\PYZus{}training}\PY{p}{,} \PY{n}{num\PYZus{}dev}\PY{p}{,} \PY{n}{replace}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}
            \PY{n}{X\PYZus{}dev} \PY{o}{=} \PY{n}{X\PYZus{}train}\PY{p}{[}\PY{n}{mask}\PY{p}{]}
            \PY{n}{y\PYZus{}dev} \PY{o}{=} \PY{n}{y\PYZus{}train}\PY{p}{[}\PY{n}{mask}\PY{p}{]}
            
            \PY{c+c1}{\PYZsh{} Preprocessing: reshape the image data into rows}
            \PY{n}{X\PYZus{}train} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{p}{(}\PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
            \PY{n}{X\PYZus{}val} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{X\PYZus{}val}\PY{p}{,} \PY{p}{(}\PY{n}{X\PYZus{}val}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
            \PY{n}{X\PYZus{}test} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{,} \PY{p}{(}\PY{n}{X\PYZus{}test}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
            \PY{n}{X\PYZus{}dev} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{X\PYZus{}dev}\PY{p}{,} \PY{p}{(}\PY{n}{X\PYZus{}dev}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
            
            \PY{c+c1}{\PYZsh{} Normalize the data: subtract the mean image}
            \PY{n}{mean\PYZus{}image} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{axis} \PY{o}{=} \PY{l+m+mi}{0}\PY{p}{)}
            \PY{n}{X\PYZus{}train} \PY{o}{\PYZhy{}}\PY{o}{=} \PY{n}{mean\PYZus{}image}
            \PY{n}{X\PYZus{}val} \PY{o}{\PYZhy{}}\PY{o}{=} \PY{n}{mean\PYZus{}image}
            \PY{n}{X\PYZus{}test} \PY{o}{\PYZhy{}}\PY{o}{=} \PY{n}{mean\PYZus{}image}
            \PY{n}{X\PYZus{}dev} \PY{o}{\PYZhy{}}\PY{o}{=} \PY{n}{mean\PYZus{}image}
            
            \PY{c+c1}{\PYZsh{} add bias dimension and transform into columns}
            \PY{n}{X\PYZus{}train} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{hstack}\PY{p}{(}\PY{p}{[}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{ones}\PY{p}{(}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}\PY{p}{]}\PY{p}{)}
            \PY{n}{X\PYZus{}val} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{hstack}\PY{p}{(}\PY{p}{[}\PY{n}{X\PYZus{}val}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{ones}\PY{p}{(}\PY{p}{(}\PY{n}{X\PYZus{}val}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}\PY{p}{]}\PY{p}{)}
            \PY{n}{X\PYZus{}test} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{hstack}\PY{p}{(}\PY{p}{[}\PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{ones}\PY{p}{(}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}\PY{p}{]}\PY{p}{)}
            \PY{n}{X\PYZus{}dev} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{hstack}\PY{p}{(}\PY{p}{[}\PY{n}{X\PYZus{}dev}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{ones}\PY{p}{(}\PY{p}{(}\PY{n}{X\PYZus{}dev}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}\PY{p}{]}\PY{p}{)}
            
            \PY{k}{return} \PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{X\PYZus{}val}\PY{p}{,} \PY{n}{y\PYZus{}val}\PY{p}{,} \PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{X\PYZus{}dev}\PY{p}{,} \PY{n}{y\PYZus{}dev}
        
        
        \PY{c+c1}{\PYZsh{} Invoke the above function to get our data.}
        \PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{X\PYZus{}val}\PY{p}{,} \PY{n}{y\PYZus{}val}\PY{p}{,} \PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{X\PYZus{}dev}\PY{p}{,} \PY{n}{y\PYZus{}dev} \PY{o}{=} \PY{n}{get\PYZus{}CIFAR10\PYZus{}data}\PY{p}{(}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Train data shape: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Train labels shape: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Validation data shape: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{X\PYZus{}val}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Validation labels shape: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{y\PYZus{}val}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Test data shape: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{X\PYZus{}test}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Test labels shape: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{dev data shape: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{X\PYZus{}dev}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{dev labels shape: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{y\PYZus{}dev}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Train data shape:  (49000, 3073)
Train labels shape:  (49000,)
Validation data shape:  (1000, 3073)
Validation labels shape:  (1000,)
Test data shape:  (1000, 3073)
Test labels shape:  (1000,)
dev data shape:  (500, 3073)
dev labels shape:  (500,)

    \end{Verbatim}

    \hypertarget{training-a-softmax-classifier.}{%
\subsection{Training a softmax
classifier.}\label{training-a-softmax-classifier.}}

The following cells will take you through building a softmax classifier.
You will implement its loss function, then subsequently train it with
gradient descent. Finally, you will choose the learning rate of gradient
descent to optimize its classification performance.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}37}]:} \PY{k+kn}{from} \PY{n+nn}{nndl} \PY{k}{import} \PY{n}{Softmax}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}39}]:} \PY{c+c1}{\PYZsh{} Declare an instance of the Softmax class.  }
         \PY{c+c1}{\PYZsh{} Weights are initialized to a random value.}
         \PY{c+c1}{\PYZsh{} Note, to keep people\PYZsq{}s first solutions consistent, we are going to use a random seed.}
         
         \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{seed}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{)}
         
         \PY{n}{num\PYZus{}classes} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{unique}\PY{p}{(}\PY{n}{y\PYZus{}train}\PY{p}{)}\PY{p}{)}
         \PY{n}{num\PYZus{}features} \PY{o}{=} \PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}
         
         \PY{n}{softmax} \PY{o}{=} \PY{n}{Softmax}\PY{p}{(}\PY{n}{dims}\PY{o}{=}\PY{p}{[}\PY{n}{num\PYZus{}classes}\PY{p}{,} \PY{n}{num\PYZus{}features}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    \hypertarget{softmax-loss}{%
\paragraph{Softmax loss}\label{softmax-loss}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}40}]:} \PY{c+c1}{\PYZsh{}\PYZsh{} Implement the loss function of the softmax using a for loop over}
         \PY{c+c1}{\PYZsh{}  the number of examples}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{y\PYZus{}train}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
         \PY{n}{loss} \PY{o}{=} \PY{n}{softmax}\PY{o}{.}\PY{n}{loss}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
6
(49000,)
(49000, 3073)
(10, 3073)
(49000, 10)

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}41}]:} \PY{n+nb}{print}\PY{p}{(}\PY{n}{loss}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
2.3277607028048966

    \end{Verbatim}

    \hypertarget{question}{%
\subsection{Question:}\label{question}}

You'll notice the loss returned by the softmax is about 2.3 (if
implemented correctly). Why does this make sense?

    \hypertarget{answer}{%
\subsection{Answer:}\label{answer}}

2.3\textasciitilde{} {[}-ln(0.1){]} which means one in the ten given
classes is what our currentl loss function is calculating. It make sense
as the hyperparameters are rondomly chosen from normal distribution
(np.random.normal{[}{]}). So 1/10 probability of choosing the right
class.

    \hypertarget{softmax-gradient}{%
\paragraph{Softmax gradient}\label{softmax-gradient}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}48}]:} \PY{c+c1}{\PYZsh{}\PYZsh{} Calculate the gradient of the softmax loss in the Softmax class.}
         \PY{c+c1}{\PYZsh{} For convenience, we\PYZsq{}ll write one function that computes the loss}
         \PY{c+c1}{\PYZsh{}   and gradient together, softmax.loss\PYZus{}and\PYZus{}grad(X, y)}
         \PY{c+c1}{\PYZsh{} You may copy and paste your loss code from softmax.loss() here, and then}
         \PY{c+c1}{\PYZsh{}   use the appropriate intermediate values to calculate the gradient.}
         
         \PY{n}{loss}\PY{p}{,} \PY{n}{grad} \PY{o}{=} \PY{n}{softmax}\PY{o}{.}\PY{n}{loss\PYZus{}and\PYZus{}grad}\PY{p}{(}\PY{n}{X\PYZus{}dev}\PY{p}{,}\PY{n}{y\PYZus{}dev}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Compare your gradient to a gradient check we wrote. }
         \PY{c+c1}{\PYZsh{} You should see relative gradient errors on the order of 1e\PYZhy{}07 or less if you implemented the gradient correctly.}
         \PY{n}{softmax}\PY{o}{.}\PY{n}{grad\PYZus{}check\PYZus{}sparse}\PY{p}{(}\PY{n}{X\PYZus{}dev}\PY{p}{,} \PY{n}{y\PYZus{}dev}\PY{p}{,} \PY{n}{grad}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
before (10, 500)
After (10, 500)
The end
numerical: -3.099203 analytic: -3.099204, relative error: 1.128370e-08
numerical: 0.799125 analytic: 0.799125, relative error: 4.943867e-08
numerical: -2.286678 analytic: -2.286678, relative error: 4.027054e-08
numerical: -0.913192 analytic: -0.913192, relative error: 1.223424e-08
numerical: 1.609597 analytic: 1.609597, relative error: 7.771982e-10
numerical: -1.768594 analytic: -1.768594, relative error: 4.054308e-08
numerical: 0.704470 analytic: 0.704470, relative error: 3.005293e-09
numerical: -0.608301 analytic: -0.608301, relative error: 5.939298e-08
numerical: 1.674017 analytic: 1.674016, relative error: 5.825551e-08
numerical: 0.056773 analytic: 0.056773, relative error: 4.050249e-07

    \end{Verbatim}

    \hypertarget{a-vectorized-version-of-softmax}{%
\subsection{A vectorized version of
Softmax}\label{a-vectorized-version-of-softmax}}

To speed things up, we will vectorize the loss and gradient
calculations. This will be helpful for stochastic gradient descent.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}49}]:} \PY{k+kn}{import} \PY{n+nn}{time}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}51}]:} \PY{c+c1}{\PYZsh{}\PYZsh{} Implement softmax.fast\PYZus{}loss\PYZus{}and\PYZus{}grad which calculates the loss and gradient}
         \PY{c+c1}{\PYZsh{}    WITHOUT using any for loops.  }
         
         \PY{c+c1}{\PYZsh{} Standard loss and gradient}
         \PY{n}{tic} \PY{o}{=} \PY{n}{time}\PY{o}{.}\PY{n}{time}\PY{p}{(}\PY{p}{)}
         \PY{n}{loss}\PY{p}{,} \PY{n}{grad} \PY{o}{=} \PY{n}{softmax}\PY{o}{.}\PY{n}{loss\PYZus{}and\PYZus{}grad}\PY{p}{(}\PY{n}{X\PYZus{}dev}\PY{p}{,} \PY{n}{y\PYZus{}dev}\PY{p}{)}
         \PY{n}{toc} \PY{o}{=} \PY{n}{time}\PY{o}{.}\PY{n}{time}\PY{p}{(}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Normal loss / grad\PYZus{}norm: }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{ / }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{ computed in }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{s}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{loss}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{norm}\PY{p}{(}\PY{n}{grad}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{fro}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,} \PY{n}{toc} \PY{o}{\PYZhy{}} \PY{n}{tic}\PY{p}{)}\PY{p}{)}
         
         \PY{n}{tic} \PY{o}{=} \PY{n}{time}\PY{o}{.}\PY{n}{time}\PY{p}{(}\PY{p}{)}
         \PY{n}{loss\PYZus{}vectorized}\PY{p}{,} \PY{n}{grad\PYZus{}vectorized} \PY{o}{=} \PY{n}{softmax}\PY{o}{.}\PY{n}{fast\PYZus{}loss\PYZus{}and\PYZus{}grad}\PY{p}{(}\PY{n}{X\PYZus{}dev}\PY{p}{,} \PY{n}{y\PYZus{}dev}\PY{p}{)}
         \PY{n}{toc} \PY{o}{=} \PY{n}{time}\PY{o}{.}\PY{n}{time}\PY{p}{(}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Vectorized loss / grad: }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{ / }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{ computed in }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{s}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{loss\PYZus{}vectorized}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{norm}\PY{p}{(}\PY{n}{grad\PYZus{}vectorized}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{fro}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,} \PY{n}{toc} \PY{o}{\PYZhy{}} \PY{n}{tic}\PY{p}{)}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} The losses should match but your vectorized implementation should be much faster.}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{difference in loss / grad: }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{ /}\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{ }\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{loss} \PY{o}{\PYZhy{}} \PY{n}{loss\PYZus{}vectorized}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{norm}\PY{p}{(}\PY{n}{grad} \PY{o}{\PYZhy{}} \PY{n}{grad\PYZus{}vectorized}\PY{p}{)}\PY{p}{)}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} You should notice a speedup with the same output.}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Normal loss / grad\_norm: 2.3380159773413673 / 335.77608489059077 computed in 0.01871013641357422s
Vectorized loss / grad: 2.3380159773413665 / 335.77608489059077 computed in 0.0025773048400878906s
difference in loss / grad: 8.881784197001252e-16 /5.006281901851313e-14 

    \end{Verbatim}

    \hypertarget{stochastic-gradient-descent}{%
\subsection{Stochastic gradient
descent}\label{stochastic-gradient-descent}}

We now implement stochastic gradient descent. This uses the same
principles of gradient descent we discussed in class, however, it
calculates the gradient by only using examples from a subset of the
training set (so each gradient calculation is faster).

    \hypertarget{question}{%
\subsection{Question:}\label{question}}

How should the softmax gradient descent training step differ from the
svm training step, if at all?

    \hypertarget{answer}{%
\subsection{Answer:}\label{answer}}

They should differ as they utilize different cost functions.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}52}]:} \PY{c+c1}{\PYZsh{} Implement softmax.train() by filling in the code to extract a batch of data}
         \PY{c+c1}{\PYZsh{} and perform the gradient step.}
         \PY{k+kn}{import} \PY{n+nn}{time}
         
         
         \PY{n}{tic} \PY{o}{=} \PY{n}{time}\PY{o}{.}\PY{n}{time}\PY{p}{(}\PY{p}{)}
         \PY{n}{loss\PYZus{}hist} \PY{o}{=} \PY{n}{softmax}\PY{o}{.}\PY{n}{train}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{learning\PYZus{}rate}\PY{o}{=}\PY{l+m+mf}{1e\PYZhy{}7}\PY{p}{,}
                               \PY{n}{num\PYZus{}iters}\PY{o}{=}\PY{l+m+mi}{1500}\PY{p}{,} \PY{n}{verbose}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
         \PY{n}{toc} \PY{o}{=} \PY{n}{time}\PY{o}{.}\PY{n}{time}\PY{p}{(}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{That took }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{s}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{toc} \PY{o}{\PYZhy{}} \PY{n}{tic}\PY{p}{)}\PY{p}{)}
         
         \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{loss\PYZus{}hist}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Iteration number}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Loss value}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
iteration 0 / 1500: loss 2.2483650583337553
iteration 100 / 1500: loss 2.1182643187377015
iteration 200 / 1500: loss 1.955351315420126
iteration 300 / 1500: loss 1.9587048512507435
iteration 400 / 1500: loss 1.8495037733293611
iteration 500 / 1500: loss 1.797760075469314
iteration 600 / 1500: loss 1.822558641450844
iteration 700 / 1500: loss 1.8674388252253464
iteration 800 / 1500: loss 1.7553670865183204
iteration 900 / 1500: loss 1.7396877079711515
iteration 1000 / 1500: loss 1.85268364487155
iteration 1100 / 1500: loss 1.7854944930427314
iteration 1200 / 1500: loss 1.7322776347927484
iteration 1300 / 1500: loss 1.838509909050927
iteration 1400 / 1500: loss 1.8741836589582033
That took 3.0625500679016113s

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \hypertarget{evaluate-the-performance-of-the-trained-softmax-classifier-on-the-validation-data.}{%
\subsubsection{Evaluate the performance of the trained softmax
classifier on the validation
data.}\label{evaluate-the-performance-of-the-trained-softmax-classifier-on-the-validation-data.}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}53}]:} \PY{c+c1}{\PYZsh{}\PYZsh{} Implement softmax.predict() and use it to compute the training and testing error.}
         
         \PY{n}{y\PYZus{}train\PYZus{}pred} \PY{o}{=} \PY{n}{softmax}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{training accuracy: }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{equal}\PY{p}{(}\PY{n}{y\PYZus{}train}\PY{p}{,}\PY{n}{y\PYZus{}train\PYZus{}pred}\PY{p}{)}\PY{p}{,} \PY{p}{)}\PY{p}{)}\PY{p}{)}
         \PY{n}{y\PYZus{}val\PYZus{}pred} \PY{o}{=} \PY{n}{softmax}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}val}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{validation accuracy: }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{equal}\PY{p}{(}\PY{n}{y\PYZus{}val}\PY{p}{,} \PY{n}{y\PYZus{}val\PYZus{}pred}\PY{p}{)}\PY{p}{)}\PY{p}{,} \PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
training accuracy: 0.38142857142857145
validation accuracy: 0.394

    \end{Verbatim}

    \hypertarget{optimize-the-softmax-classifier}{%
\subsection{Optimize the softmax
classifier}\label{optimize-the-softmax-classifier}}

You may copy and paste your optimization code from the SVM here.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}54}]:} \PY{n}{np}\PY{o}{.}\PY{n}{finfo}\PY{p}{(}\PY{n+nb}{float}\PY{p}{)}\PY{o}{.}\PY{n}{eps}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}54}]:} 2.220446049250313e-16
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}55}]:} \PY{c+c1}{\PYZsh{} ================================================================ \PYZsh{}}
         \PY{c+c1}{\PYZsh{} YOUR CODE HERE:}
         \PY{c+c1}{\PYZsh{}   Train the Softmax classifier with different learning rates and }
         \PY{c+c1}{\PYZsh{}     evaluate on the validation data.}
         \PY{c+c1}{\PYZsh{}   Report:}
         \PY{c+c1}{\PYZsh{}     \PYZhy{} The best learning rate of the ones you tested.  }
         \PY{c+c1}{\PYZsh{}     \PYZhy{} The best validation accuracy corresponding to the best validation error.}
         \PY{c+c1}{\PYZsh{}}
         \PY{c+c1}{\PYZsh{}   Select the SVM that achieved the best validation error and report}
         \PY{c+c1}{\PYZsh{}     its error rate on the test set.}
         \PY{c+c1}{\PYZsh{} ================================================================ \PYZsh{}}
         \PY{n}{rates} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{10}\PY{o}{*}\PY{o}{*}\PY{n}{i} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{12}\PY{p}{,}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{]}
         
         \PY{n}{Best} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]} \PY{c+c1}{\PYZsh{} rate, train accuracy, test accuracy}
         \PY{k}{for} \PY{n}{rate} \PY{o+ow}{in} \PY{n}{rates}\PY{p}{:}
             \PY{n}{softmax}\PY{o}{.}\PY{n}{train}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{learning\PYZus{}rate}\PY{o}{=}\PY{n}{rate}\PY{p}{,} \PY{n}{num\PYZus{}iters}\PY{o}{=}\PY{l+m+mi}{1500}\PY{p}{,} \PY{n}{verbose}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}
             \PY{n}{y\PYZus{}val\PYZus{}pred} \PY{o}{=} \PY{n}{softmax}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}val}\PY{p}{)}
             
             \PY{k}{if} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{equal}\PY{p}{(}\PY{n}{y\PYZus{}val}\PY{p}{,} \PY{n}{y\PYZus{}val\PYZus{}pred}\PY{p}{)}\PY{p}{)} \PY{o}{\PYZgt{}} \PY{n}{Best}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{:} 
                 \PY{n}{Best}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{=} \PY{n}{rate}
                 \PY{n}{Best}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{equal}\PY{p}{(}\PY{n}{y\PYZus{}val}\PY{p}{,} \PY{n}{y\PYZus{}val\PYZus{}pred}\PY{p}{)}\PY{p}{)}
             \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{rate:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{rate}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{validation accuracy: }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{equal}\PY{p}{(}\PY{n}{y\PYZus{}val}\PY{p}{,} \PY{n}{y\PYZus{}val\PYZus{}pred}\PY{p}{)}\PY{p}{)}\PY{p}{,} \PY{p}{)}\PY{p}{)}
             
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{*}\PY{l+m+mi}{20}\PY{p}{)}
         
         \PY{n}{softmax}\PY{o}{.}\PY{n}{train}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{learning\PYZus{}rate}\PY{o}{=}\PY{n}{Best}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{num\PYZus{}iters}\PY{o}{=}\PY{l+m+mi}{1500}\PY{p}{,} \PY{n}{verbose}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}
         \PY{n}{y\PYZus{}test\PYZus{}pred} \PY{o}{=} \PY{n}{softmax}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}
         \PY{n}{Best}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{equal}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}test\PYZus{}pred}\PY{p}{)}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Best rate is}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{Best}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Test Set Error Rate is}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+m+mi}{1}\PY{o}{\PYZhy{}}\PY{n}{Best}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)} 
         \PY{c+c1}{\PYZsh{} ================================================================ \PYZsh{}}
         \PY{c+c1}{\PYZsh{} END YOUR CODE HERE}
         \PY{c+c1}{\PYZsh{} ================================================================ \PYZsh{}}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
rate: 1e-12 validation accuracy: 0.133
rate: 1e-11 validation accuracy: 0.059
rate: 1e-10 validation accuracy: 0.134
rate: 1e-09 validation accuracy: 0.154
rate: 1e-08 validation accuracy: 0.301
rate: 1e-07 validation accuracy: 0.394
rate: 1e-06 validation accuracy: 0.412
rate: 1e-05 validation accuracy: 0.301
rate: 0.0001 validation accuracy: 0.272

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
/Users/ApplePro/Desktop/School/GradSchool/Courses/C247/hw2/nndl/softmax.py:142: RuntimeWarning: divide by zero encountered in log
  loss = np.sum(  -np.log(a\_exp[np.arange(a.shape[0]), y] / np.sum(a\_exp, axis = 1)) )

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
rate: 0.001 validation accuracy: 0.285
rate: 0.01 validation accuracy: 0.268
--------------------
Best rate is 1e-06 Test Set Error Rate is 0.615

    \end{Verbatim}

    \hypertarget{softmax.py}{%
\section{softmax.py}\label{softmax.py}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
        
        \PY{k}{class} \PY{n+nc}{Softmax}\PY{p}{(}\PY{n+nb}{object}\PY{p}{)}\PY{p}{:}
        
          \PY{k}{def} \PY{n+nf}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{dims}\PY{o}{=}\PY{p}{[}\PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{3073}\PY{p}{]}\PY{p}{)}\PY{p}{:}
            \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{init\PYZus{}weights}\PY{p}{(}\PY{n}{dims}\PY{o}{=}\PY{n}{dims}\PY{p}{)}
        
          \PY{k}{def} \PY{n+nf}{init\PYZus{}weights}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{dims}\PY{p}{)}\PY{p}{:}
            \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{	Initializes the weight matrix of the Softmax classifier.  }
        \PY{l+s+sd}{	Note that it has shape (C, D) where C is the number of }
        \PY{l+s+sd}{	classes and D is the feature size.}
        \PY{l+s+sd}{	\PYZdq{}\PYZdq{}\PYZdq{}}
            \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{W} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{normal}\PY{p}{(}\PY{n}{size}\PY{o}{=}\PY{n}{dims}\PY{p}{)} \PY{o}{*} \PY{l+m+mf}{0.0001}
            
        
          \PY{k}{def} \PY{n+nf}{loss}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{)}\PY{p}{:}
            \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{    Calculates the softmax loss.}
        \PY{l+s+sd}{  }
        \PY{l+s+sd}{    Inputs have dimension D, there are C classes, and we operate on minibatches}
        \PY{l+s+sd}{    of N examples.}
        \PY{l+s+sd}{  }
        \PY{l+s+sd}{    Inputs:}
        \PY{l+s+sd}{    \PYZhy{} X: A numpy array of shape (N, D) containing a minibatch of data.}
        \PY{l+s+sd}{    \PYZhy{} y: A numpy array of shape (N,) containing training labels; y[i] = c means}
        \PY{l+s+sd}{      that X[i] has label c, where 0 \PYZlt{}= c \PYZlt{} C.}
        \PY{l+s+sd}{  }
        \PY{l+s+sd}{    Returns a tuple of:}
        \PY{l+s+sd}{    \PYZhy{} loss as single float}
        \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
        
            \PY{c+c1}{\PYZsh{} Initialize the loss to zero.}
            \PY{n}{loss} \PY{o}{=} \PY{l+m+mf}{0.0}
        
            \PY{c+c1}{\PYZsh{} ================================================================ \PYZsh{}}
            \PY{c+c1}{\PYZsh{} YOUR CODE HERE:}
        	\PY{c+c1}{\PYZsh{}   Calculate the normalized softmax loss.  Store it as the variable loss.}
            \PY{c+c1}{\PYZsh{}   (That is, calculate the sum of the losses of all the training }
            \PY{c+c1}{\PYZsh{}   set margins, and then normalize the loss by the number of }
        	\PY{c+c1}{\PYZsh{}	training examples.)}
            \PY{c+c1}{\PYZsh{} ================================================================ \PYZsh{}}
            \PY{n}{a} \PY{o}{=} \PY{n}{X}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{W}\PY{o}{.}\PY{n}{T}\PY{p}{)}
            \PY{c+c1}{\PYZsh{}print(y.shape)}
            \PY{c+c1}{\PYZsh{}print(X.shape)}
            \PY{c+c1}{\PYZsh{}print(self.W.shape)}
            \PY{c+c1}{\PYZsh{}print(a.shape)}
            \PY{n}{i} \PY{o}{=} \PY{l+m+mi}{0}
            \PY{k}{for} \PY{n}{row} \PY{o+ow}{in} \PY{n}{a}\PY{p}{:}    
                \PY{n}{row} \PY{o}{\PYZhy{}}\PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{n}{row}\PY{p}{)} \PY{c+c1}{\PYZsh{}To avoid overflow }
                \PY{n}{loss} \PY{o}{+}\PY{o}{=} \PY{o}{\PYZhy{}} \PY{n}{np}\PY{o}{.}\PY{n}{log}\PY{p}{(} \PY{n}{np}\PY{o}{.}\PY{n}{exp}\PY{p}{(}\PY{n}{row}\PY{p}{[}\PY{n}{y}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{]}\PY{p}{)} \PY{o}{/}\PY{n+nb}{sum}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{exp}\PY{p}{(}\PY{n}{row}\PY{p}{)}\PY{p}{)} \PY{p}{)}
                \PY{n}{i} \PY{o}{=} \PY{n}{i}\PY{o}{+}\PY{l+m+mi}{1}   
                
            \PY{n}{loss} \PY{o}{=} \PY{n}{loss}\PY{o}{/}\PY{n}{y}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
            
            
            \PY{c+c1}{\PYZsh{} ================================================================ \PYZsh{}}
            \PY{c+c1}{\PYZsh{} END YOUR CODE HERE}
            \PY{c+c1}{\PYZsh{} ================================================================ \PYZsh{}}
        
            \PY{k}{return} \PY{n}{loss}
        
          \PY{k}{def} \PY{n+nf}{loss\PYZus{}and\PYZus{}grad}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{)}\PY{p}{:}
            \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{	Same as self.loss(X, y), except that it also returns the gradient.}
        
        \PY{l+s+sd}{	Output: grad \PYZhy{}\PYZhy{} a matrix of the same dimensions as W containing }
        \PY{l+s+sd}{		the gradient of the loss with respect to W.}
        \PY{l+s+sd}{	\PYZdq{}\PYZdq{}\PYZdq{}}
            
            \PY{c+c1}{\PYZsh{} Initialize the loss and gradient to zero.}
            \PY{n}{loss} \PY{o}{=} \PY{l+m+mf}{0.0}
            \PY{n}{grad} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros\PYZus{}like}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{W}\PY{p}{)}
          
            \PY{c+c1}{\PYZsh{} ================================================================ \PYZsh{}}
            \PY{c+c1}{\PYZsh{} YOUR CODE HERE:}
        	\PY{c+c1}{\PYZsh{}   Calculate the softmax loss and the gradient. Store the gradient}
        	\PY{c+c1}{\PYZsh{}   as the variable grad.}
            \PY{c+c1}{\PYZsh{} ================================================================ \PYZsh{}}
            \PY{c+c1}{\PYZsh{} dL/dW = dL/da*da/dW}
            \PY{c+c1}{\PYZsh{} dL/da = Score \PYZhy{} 1(y == i)}
            \PY{c+c1}{\PYZsh{} da/dW = X\PYZsq{}}
            \PY{n}{a} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{W}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{X}\PY{o}{.}\PY{n}{T}\PY{p}{)} \PY{c+c1}{\PYZsh{}  numOfClass * numOfSample}
            \PY{n}{a\PYZus{}exp} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{exp}\PY{p}{(}\PY{n}{a}\PY{p}{)}
            \PY{n}{Score} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros\PYZus{}like}\PY{p}{(}\PY{n}{a\PYZus{}exp}\PY{p}{)}
            
            \PY{k}{for} \PY{n}{j} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{Score}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{:}
                \PY{n}{Score}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{n}{j}\PY{p}{]} \PY{o}{=} \PY{n}{a\PYZus{}exp}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{n}{j}\PY{p}{]}\PY{o}{/}\PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{a\PYZus{}exp}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{n}{j}\PY{p}{]}\PY{p}{)}
                
            \PY{n}{Score}\PY{p}{[}\PY{n}{y}\PY{p}{,} \PY{n+nb}{range}\PY{p}{(}\PY{n}{Score}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{]} \PY{o}{\PYZhy{}}\PY{o}{=}\PY{l+m+mi}{1} 
            \PY{n}{dLda} \PY{o}{=} \PY{n}{Score}
            \PY{n}{grad} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{dLda}\PY{p}{,}\PY{n}{X}\PY{p}{)}
            
            \PY{n}{grad} \PY{o}{/}\PY{o}{=} \PY{n}{y}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
            \PY{n}{loss} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{loss}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{)}
            \PY{c+c1}{\PYZsh{} ================================================================ \PYZsh{}}
            \PY{c+c1}{\PYZsh{} END YOUR CODE HERE}
            \PY{c+c1}{\PYZsh{} ================================================================ \PYZsh{}}
        
            \PY{k}{return} \PY{n}{loss}\PY{p}{,} \PY{n}{grad}
        
          \PY{k}{def} \PY{n+nf}{grad\PYZus{}check\PYZus{}sparse}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{your\PYZus{}grad}\PY{p}{,} \PY{n}{num\PYZus{}checks}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{,} \PY{n}{h}\PY{o}{=}\PY{l+m+mf}{1e\PYZhy{}5}\PY{p}{)}\PY{p}{:}
            \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{    sample a few random elements and only return numerical}
        \PY{l+s+sd}{    in these dimensions.}
        \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
          
            \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{n}{num\PYZus{}checks}\PY{p}{)}\PY{p}{:}
              \PY{n}{ix} \PY{o}{=} \PY{n+nb}{tuple}\PY{p}{(}\PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{randint}\PY{p}{(}\PY{n}{m}\PY{p}{)} \PY{k}{for} \PY{n}{m} \PY{o+ow}{in} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{W}\PY{o}{.}\PY{n}{shape}\PY{p}{]}\PY{p}{)}
          
              \PY{n}{oldval} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{W}\PY{p}{[}\PY{n}{ix}\PY{p}{]}
              \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{W}\PY{p}{[}\PY{n}{ix}\PY{p}{]} \PY{o}{=} \PY{n}{oldval} \PY{o}{+} \PY{n}{h} \PY{c+c1}{\PYZsh{} increment by h}
              \PY{n}{fxph} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{loss}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{)}
              \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{W}\PY{p}{[}\PY{n}{ix}\PY{p}{]} \PY{o}{=} \PY{n}{oldval} \PY{o}{\PYZhy{}} \PY{n}{h} \PY{c+c1}{\PYZsh{} decrement by h}
              \PY{n}{fxmh} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{loss}\PY{p}{(}\PY{n}{X}\PY{p}{,}\PY{n}{y}\PY{p}{)} \PY{c+c1}{\PYZsh{} evaluate f(x \PYZhy{} h)}
              \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{W}\PY{p}{[}\PY{n}{ix}\PY{p}{]} \PY{o}{=} \PY{n}{oldval} \PY{c+c1}{\PYZsh{} reset}
          
              \PY{n}{grad\PYZus{}numerical} \PY{o}{=} \PY{p}{(}\PY{n}{fxph} \PY{o}{\PYZhy{}} \PY{n}{fxmh}\PY{p}{)} \PY{o}{/} \PY{p}{(}\PY{l+m+mi}{2} \PY{o}{*} \PY{n}{h}\PY{p}{)}
              \PY{n}{grad\PYZus{}analytic} \PY{o}{=} \PY{n}{your\PYZus{}grad}\PY{p}{[}\PY{n}{ix}\PY{p}{]}
              \PY{n}{rel\PYZus{}error} \PY{o}{=} \PY{n+nb}{abs}\PY{p}{(}\PY{n}{grad\PYZus{}numerical} \PY{o}{\PYZhy{}} \PY{n}{grad\PYZus{}analytic}\PY{p}{)} \PY{o}{/} \PY{p}{(}\PY{n+nb}{abs}\PY{p}{(}\PY{n}{grad\PYZus{}numerical}\PY{p}{)} \PY{o}{+} \PY{n+nb}{abs}\PY{p}{(}\PY{n}{grad\PYZus{}analytic}\PY{p}{)}\PY{p}{)}
              \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{numerical: }\PY{l+s+si}{\PYZpc{}f}\PY{l+s+s1}{ analytic: }\PY{l+s+si}{\PYZpc{}f}\PY{l+s+s1}{, relative error: }\PY{l+s+si}{\PYZpc{}e}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{p}{(}\PY{n}{grad\PYZus{}numerical}\PY{p}{,} \PY{n}{grad\PYZus{}analytic}\PY{p}{,} \PY{n}{rel\PYZus{}error}\PY{p}{)}\PY{p}{)}
        
          \PY{k}{def} \PY{n+nf}{fast\PYZus{}loss\PYZus{}and\PYZus{}grad}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{)}\PY{p}{:}
            \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{    A vectorized implementation of loss\PYZus{}and\PYZus{}grad. It shares the same}
        \PY{l+s+sd}{	inputs and ouptuts as loss\PYZus{}and\PYZus{}grad.}
        \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
            \PY{n}{loss} \PY{o}{=} \PY{l+m+mf}{0.0}
            \PY{n}{grad} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{W}\PY{o}{.}\PY{n}{shape}\PY{p}{)} \PY{c+c1}{\PYZsh{} initialize the gradient as zero}
          
            \PY{c+c1}{\PYZsh{} ================================================================ \PYZsh{}}
            \PY{c+c1}{\PYZsh{} YOUR CODE HERE:}
        	\PY{c+c1}{\PYZsh{}   Calculate the softmax loss and gradient WITHOUT any for loops.}
            \PY{c+c1}{\PYZsh{} ================================================================ \PYZsh{}}
            \PY{n}{a} \PY{o}{=} \PY{n}{X}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{W}\PY{o}{.}\PY{n}{T}\PY{p}{)} \PY{c+c1}{\PYZsh{}   numOfSample *numOfClass }
            \PY{n}{a} \PY{o}{=} \PY{p}{(}\PY{n}{a}\PY{o}{.}\PY{n}{T} \PY{o}{\PYZhy{}} \PY{n}{np}\PY{o}{.}\PY{n}{amax}\PY{p}{(}\PY{n}{a}\PY{p}{,}\PY{n}{axis} \PY{o}{=} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}\PY{o}{.}\PY{n}{T}
            \PY{n}{num\PYZus{}train} \PY{o}{=} \PY{n}{y}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
            \PY{n}{a\PYZus{}exp} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{exp}\PY{p}{(}\PY{n}{a}\PY{p}{)}
            \PY{n}{Score} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros\PYZus{}like}\PY{p}{(}\PY{n}{a\PYZus{}exp}\PY{p}{)}
            \PY{n}{Score} \PY{o}{=} \PY{n}{a\PYZus{}exp} \PY{o}{/} \PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{a\PYZus{}exp}\PY{p}{,} \PY{n}{axis} \PY{o}{=} \PY{l+m+mi}{1}\PY{p}{,} \PY{n}{keepdims} \PY{o}{=} \PY{k+kc}{True}\PY{p}{)} 
            \PY{c+c1}{\PYZsh{}loss += \PYZhy{} np.log( np.exp(row[y[i]]) /sum(np.exp(row)) )}
            \PY{n}{loss} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}  \PY{o}{\PYZhy{}}\PY{n}{np}\PY{o}{.}\PY{n}{log}\PY{p}{(}\PY{n}{a\PYZus{}exp}\PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{n}{a}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{,} \PY{n}{y}\PY{p}{]} \PY{o}{/} \PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{a\PYZus{}exp}\PY{p}{,} \PY{n}{axis} \PY{o}{=} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)} \PY{p}{)}
            
            
            \PY{n}{Score}\PY{p}{[}\PY{n+nb}{range}\PY{p}{(}\PY{n}{num\PYZus{}train}\PY{p}{)}\PY{p}{,}\PY{n}{y}\PY{p}{]} \PY{o}{\PYZhy{}}\PY{o}{=} \PY{l+m+mi}{1}
            \PY{n}{dLda} \PY{o}{=} \PY{n}{Score}
            \PY{n}{grad} \PY{o}{=} \PY{n}{dLda}\PY{o}{.}\PY{n}{T}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{X}\PY{p}{)}
            \PY{n}{grad} \PY{o}{/}\PY{o}{=} \PY{n}{num\PYZus{}train} \PY{c+c1}{\PYZsh{} NumOfClass * NumOfDimension}
            
            \PY{n}{loss} \PY{o}{=} \PY{n}{loss}\PY{o}{/}\PY{n}{num\PYZus{}train}
            
            
            \PY{c+c1}{\PYZsh{} ================================================================ \PYZsh{}}
            \PY{c+c1}{\PYZsh{} END YOUR CODE HERE}
            \PY{c+c1}{\PYZsh{} ================================================================ \PYZsh{}}
        
            \PY{k}{return} \PY{n}{loss}\PY{p}{,} \PY{n}{grad}
        
          \PY{k}{def} \PY{n+nf}{train}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{learning\PYZus{}rate}\PY{o}{=}\PY{l+m+mf}{1e\PYZhy{}3}\PY{p}{,} \PY{n}{num\PYZus{}iters}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{,}
                    \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{200}\PY{p}{,} \PY{n}{verbose}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}\PY{p}{:}
            \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{    Train this linear classifier using stochastic gradient descent.}
        
        \PY{l+s+sd}{    Inputs:}
        \PY{l+s+sd}{    \PYZhy{} X: A numpy array of shape (N, D) containing training data; there are N}
        \PY{l+s+sd}{      training samples each of dimension D.}
        \PY{l+s+sd}{    \PYZhy{} y: A numpy array of shape (N,) containing training labels; y[i] = c}
        \PY{l+s+sd}{      means that X[i] has label 0 \PYZlt{}= c \PYZlt{} C for C classes.}
        \PY{l+s+sd}{    \PYZhy{} learning\PYZus{}rate: (float) learning rate for optimization.}
        \PY{l+s+sd}{    \PYZhy{} num\PYZus{}iters: (integer) number of steps to take when optimizing}
        \PY{l+s+sd}{    \PYZhy{} batch\PYZus{}size: (integer) number of training examples to use at each step.}
        \PY{l+s+sd}{    \PYZhy{} verbose: (boolean) If true, print progress during optimization.}
        
        \PY{l+s+sd}{    Outputs:}
        \PY{l+s+sd}{    A list containing the value of the loss function at each training iteration.}
        \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
            \PY{n}{num\PYZus{}train}\PY{p}{,} \PY{n}{dim} \PY{o}{=} \PY{n}{X}\PY{o}{.}\PY{n}{shape}
            \PY{n}{num\PYZus{}classes} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{n}{y}\PY{p}{)} \PY{o}{+} \PY{l+m+mi}{1} \PY{c+c1}{\PYZsh{} assume y takes values 0...K\PYZhy{}1 where K is number of classes}
        
            \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{init\PYZus{}weights}\PY{p}{(}\PY{n}{dims}\PY{o}{=}\PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{n}{y}\PY{p}{)} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{,} \PY{n}{X}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{]}\PY{p}{)}	\PY{c+c1}{\PYZsh{} initializes the weights of self.W}
        
            \PY{c+c1}{\PYZsh{} Run stochastic gradient descent to optimize W}
            \PY{n}{loss\PYZus{}history} \PY{o}{=} \PY{p}{[}\PY{p}{]}
        
            \PY{k}{for} \PY{n}{it} \PY{o+ow}{in} \PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{n}{num\PYZus{}iters}\PY{p}{)}\PY{p}{:}
              \PY{n}{X\PYZus{}batch} \PY{o}{=} \PY{k+kc}{None}
              \PY{n}{y\PYZus{}batch} \PY{o}{=} \PY{k+kc}{None}
        
              \PY{c+c1}{\PYZsh{} ================================================================ \PYZsh{}}
              \PY{c+c1}{\PYZsh{} YOUR CODE HERE:}
              \PY{c+c1}{\PYZsh{}   Sample batch\PYZus{}size elements from the training data for use in }
              \PY{c+c1}{\PYZsh{}	  gradient descent.  After sampling,}
              \PY{c+c1}{\PYZsh{}     \PYZhy{} X\PYZus{}batch should have shape: (dim, batch\PYZus{}size)}
        	  \PY{c+c1}{\PYZsh{}     \PYZhy{} y\PYZus{}batch should have shape: (batch\PYZus{}size,)}
        	  \PY{c+c1}{\PYZsh{}   The indices should be randomly generated to reduce correlations}
        	  \PY{c+c1}{\PYZsh{}   in the dataset.  Use np.random.choice.  It\PYZsq{}s okay to sample with}
        	  \PY{c+c1}{\PYZsh{}   replacement.}
              \PY{c+c1}{\PYZsh{} ================================================================ \PYZsh{}}
              \PY{n}{index} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{choice}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{n}{num\PYZus{}train}\PY{p}{)}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{p}{)}
              \PY{n}{X\PYZus{}batch} \PY{o}{=} \PY{n}{X}\PY{p}{[}\PY{n}{index}\PY{p}{]}
              \PY{n}{y\PYZus{}batch} \PY{o}{=} \PY{n}{y}\PY{p}{[}\PY{n}{index}\PY{p}{]}
              \PY{c+c1}{\PYZsh{} ================================================================ \PYZsh{}}
              \PY{c+c1}{\PYZsh{} END YOUR CODE HERE}
              \PY{c+c1}{\PYZsh{} ================================================================ \PYZsh{}}
        
              \PY{c+c1}{\PYZsh{} evaluate loss and gradient}
              \PY{n}{loss}\PY{p}{,} \PY{n}{grad} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fast\PYZus{}loss\PYZus{}and\PYZus{}grad}\PY{p}{(}\PY{n}{X\PYZus{}batch}\PY{p}{,} \PY{n}{y\PYZus{}batch}\PY{p}{)}
              \PY{n}{loss\PYZus{}history}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{loss}\PY{p}{)}
        
              \PY{c+c1}{\PYZsh{} ================================================================ \PYZsh{}}
              \PY{c+c1}{\PYZsh{} YOUR CODE HERE:}
              \PY{c+c1}{\PYZsh{}   Update the parameters, self.W, with a gradient step }
              \PY{c+c1}{\PYZsh{} ================================================================ \PYZsh{}}
              \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{W} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{W} \PY{o}{\PYZhy{}} \PY{n}{grad}\PY{o}{*} \PY{n}{learning\PYZus{}rate}
        
        	  \PY{c+c1}{\PYZsh{} ================================================================ \PYZsh{}}
              \PY{c+c1}{\PYZsh{} END YOUR CODE HERE}
              \PY{c+c1}{\PYZsh{} ================================================================ \PYZsh{}}
        
              \PY{k}{if} \PY{n}{verbose} \PY{o+ow}{and} \PY{n}{it} \PY{o}{\PYZpc{}} \PY{l+m+mi}{100} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{:}
                \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{iteration }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{ / }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{: loss }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{it}\PY{p}{,} \PY{n}{num\PYZus{}iters}\PY{p}{,} \PY{n}{loss}\PY{p}{)}\PY{p}{)}
        
            \PY{k}{return} \PY{n}{loss\PYZus{}history}
        
          \PY{k}{def} \PY{n+nf}{predict}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{X}\PY{p}{)}\PY{p}{:}
            \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{    Inputs:}
        \PY{l+s+sd}{    \PYZhy{} X: N x D array of training data. Each row is a D\PYZhy{}dimensional point.}
        
        \PY{l+s+sd}{    Returns:}
        \PY{l+s+sd}{    \PYZhy{} y\PYZus{}pred: Predicted labels for the data in X. y\PYZus{}pred is a 1\PYZhy{}dimensional}
        \PY{l+s+sd}{      array of length N, and each element is an integer giving the predicted}
        \PY{l+s+sd}{      class.}
        \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
            \PY{n}{y\PYZus{}pred} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{n}{X}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
            \PY{c+c1}{\PYZsh{} ================================================================ \PYZsh{}}
            \PY{c+c1}{\PYZsh{} YOUR CODE HERE:}
            \PY{c+c1}{\PYZsh{}   Predict the labels given the training data.}
            \PY{c+c1}{\PYZsh{} ================================================================ \PYZsh{}}
            \PY{n}{y\PYZus{}pred} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{X}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{W}\PY{o}{.}\PY{n}{T}\PY{p}{)}\PY{p}{,}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
            \PY{c+c1}{\PYZsh{} ================================================================ \PYZsh{}}
            \PY{c+c1}{\PYZsh{} END YOUR CODE HERE}
            \PY{c+c1}{\PYZsh{} ================================================================ \PYZsh{}}
        
            \PY{k}{return} \PY{n}{y\PYZus{}pred}
\end{Verbatim}



    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
